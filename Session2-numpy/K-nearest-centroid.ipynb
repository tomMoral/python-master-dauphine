{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions de bases sur `numpy` \n",
    "\n",
    "**Exercice 1 :** (*Créer un tableau avec une liste*)  \n",
    "* Créer un simple tableau à 2 dimensions (contenant les éléments que vous voulez).  \n",
    "* Utiliser les fonctions `len()`, `np.shape()` sur votre tableau. Comment sont elles reliées? comment est ce relié à l'attribut `ndim`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2 :** (*Indexation des donnés*)  \n",
    "* Modifier le tableau `v` pour que son 3e élément soit égale à 0.\n",
    "* Modifier le tableau `v` pour que son dernier élément soit égale à 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3 :** (*Générer des tableaux de constantes*)  \n",
    "Utiliser les fonctions `np.zeros`, `np.ones` et `np.eye` pour répondre au question suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer un tableau de 0 de taille 3, puis de taille (2, 3)\n",
    "v0 =\n",
    "M0 =\n",
    "print(\"0 vector of size: {}\\n\".format(v0.shape), v0)\n",
    "print(\"0 matrix of size: {}\\n\".format(M0.shape), M0)\n",
    "assert v0.shape == (3,)\n",
    "assert M0.shape == (2, 3)\n",
    "assert np.all(M0 == 0)\n",
    "\n",
    "# Générer un tableau de 1 de taille 10, puis de taille 3x4x5\n",
    "v1 =\n",
    "M1 =\n",
    "print(\"1 vector of size: {}\\n\".format(v1.shape), v1)\n",
    "print(\"1 tensor of size: {}\\n\".format(M1.shape), M1)\n",
    "assert v1.shape == (10,)\n",
    "assert M1.shape == (2, 3, 2)\n",
    "assert np.all(M1 == 1)\n",
    "\n",
    "# Générer la matrice identité de taille 4x4\n",
    "I =\n",
    "print(\"Id matrix of size: {}\\n\".format(I.shape), I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4 :** (*Générer des intervals*)  \n",
    "Recréer les mêmes intervals avec la fonction `np.linspace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec linspace, le début et la fin SONT inclus\n",
    "# np.linspace?\n",
    "\n",
    "# Créer un tableau avec les valeurs paires de 0 à 20:\n",
    "l1 =\n",
    "assert np.allclose(l1, x1)\n",
    "\n",
    "# Créer un tableau avec les valeurs entre -1 et 1, tous les .1:\n",
    "l2 =\n",
    "assert np.allclose(l2, x2)\n",
    "\n",
    "# Créer un tableau avec les valeurs de 100 à 0:\n",
    "l3 =\n",
    "assert np.allclose(l3, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing des tableaux\n",
    "\n",
    "**Exercice 5 :** (*le plateau d'échec*)\n",
    "\n",
    "Créez un tableau de zéros et le remplir pour obtenir un motif de plateau d'échec de dimension 8x8.\n",
    "<img src=\"checkerboard.svg\" width=300, height=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 6 :** (*Fancy indexing*)  \n",
    "* Généré un tableau contenant  contenant 100 éléments tirés au hasard selon $\\mathcal U([0, 1])$ (~ `np.rand`).\n",
    "* En utilisant l'indexation avancée, sélectionnez au hasard avec répétition 10 éléments de ce tableau.\n",
    "\n",
    "(*Astuce: `np.random.randint(max_int, size=n)` génère n nombres au hasard de 0 à `max_int`*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opérations sur les tableaux\n",
    "\n",
    "#### Opérations terme-à-terme\n",
    "\n",
    "**Exercice 7 :** (*Courbe paramétrique*)  \n",
    "Tracer la courbe $x = cos(t)$ et $y=sin(t)$ pour $t \\in [0, 2\\pi]$.  \n",
    "On pourra utiliser la constante $\\pi=$`np.pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "Que donnerait les deux cas suivant?\n",
    "\n",
    "```\n",
    "Image  (3d array): 256 x 256 x 3\n",
    "Scale  (1d array):             3\n",
    "Result (3d array): \n",
    "\n",
    "A      (4d array):  8 x 1 x 6 x 1\n",
    "B      (3d array):      7 x 1 x 5\n",
    "Result (4d array):  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 8 :** (*Opération terme-à-terme*)\n",
    "\n",
    "Sans utiliser de boucles (`for/while`) :\n",
    "\n",
    " * Créer une matrice (5x6) aléatoire\n",
    " * Remplacer une colonne sur deux par sa valeur moins le double de la colonne suivante\n",
    " * Remplacer les valeurs négatives par 0 en utilisant un masque binaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Plus proche centroid\n",
    "\n",
    "Nous allons maintenant implémenter l'agorithme des [plus proches centroids](https://en.wikipedia.org/wiki/Nearest_centroid_classifier) en pure `numpy`, depuis la génération de donnée jusqu'a la visualisation du résultat.  \n",
    "L'idée de cette algorithme est simple:\n",
    "* A partir de l'ensemble d'entrainement $(X^i, y^i)$, calculer pour chaque classe la moyenne des points de cette classe, *i.e.* $\\bar X_l = \\frac{1}{|C_l|} \\sum_{i\\in C_l} X^i$ où $C_l$ est les indices des points appartenant à la classe $l$.\n",
    "* Pour un point de test $X$, lui assigner la classe $y = \\arg\\min_l \\|X - \\bar X_l\\|_2$.\n",
    "\n",
    "Les étapes à réaliser sont:\n",
    "\n",
    "1) Générer des nuages de points en 2D selon des lois normales de moyennes différentes pour chaque classe.\n",
    "\n",
    "2) Visualiser ces nuages de points\n",
    "\n",
    "3) Diviser les données entre un ensemble d'entrainement et un ensemble de test.\n",
    "\n",
    "4) Entrainer le modèle: calculer les centroids pour chacune des classes\n",
    "\n",
    "5) Utiliser le modèle: prédire les classes des points dans l'ensemble de test.\n",
    "\n",
    "6) Évaluer le modèle: donner la précision et le rappel du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Génération des données\n",
    "\n",
    "On va maintenant utiliser les fonctions que l'on a vu précédement pour générer des données pour notre algorithme de plus proche centroid.  \n",
    "* On va créer des données avec 2 classes : $\\{0, 1\\}$.\n",
    "* Pour la classe $i$, on générera les X associés dans $\\mathbb R^2$ selon la loi normale $\\mathcal N(\\mu_i, \\pmb I_2)$.\n",
    "\n",
    "\n",
    "Procédez selon les étapes suivantes:\n",
    "* Créer 2 vecteurs avec en dimension 2 correspondant aux moyennes de chacune des classes. On commencera avec de moyenne deterministe $\\mu_0 = 0$ et $\\mu_1 = [rho, rho]$ avec $rho=3$.\n",
    "* Concaténer `mu0` et `mu1` en un vecteur `mu` de taille `n_classes` x `n_dims` (Indice: utiliser `np.concatenate`).\n",
    "* Générer un vecteur de classes `y` de taille `n_points=1000` avec des valeurs uniforme dans $\\{0, 1\\}$.\n",
    "* Générer un jeu de donné $X$ avec `n_points=1000` points en dimension `n_dim=2` selon la loi $\\mathcal N(0, \\pmb I_2)$. (*Rappel:* tiré selon une loi normal de covariance $\\pmb I_2$ revient à tirer chaque coordonnée selon une loi normale indépendante.)\n",
    "* Ajouter à chaque point la moyenne correspondant à sa classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définion des constantes\n",
    "rho = 3\n",
    "n_dim = 2\n",
    "n_points = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vous de jouer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Visualisation des données\n",
    "\n",
    "Utiliser la fonction `plot_data` pour visualiser les données.  \n",
    "* Afficher les donnés $(X, y)$, avec l'option `alpha=.1`.\n",
    "* Afficher les moyennes avec les mêmes couleurs et l'option `s=256, alpha=1`.\n",
    "Que fait la fonction scatter?\n",
    "\n",
    "Bonus:\n",
    "\n",
    "* Visualiser la fonction de répartition des données à l'aide de `plt.contourf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS = ['^', 's', 'o', 'h', '>', 'v', '*', '+', 'x', '<']\n",
    "CMAP = plt.get_cmap('tab10', 10)\n",
    "\n",
    "def plot_data(X, y, alpha=1, s=36):\n",
    "    \"\"\"Plot the data with color depending on the classes.\n",
    "    \n",
    "    Parameter\n",
    "    =========\n",
    "    X: ndarray, shape (n_points, n_dim)\n",
    "        Point to display\n",
    "    y : ndarray, shape (n_points,)\n",
    "        Classes for each point\n",
    "    alpha: float\n",
    "        Opacity of the points\n",
    "    s: int\n",
    "        Size of the points in pt^2.\n",
    "    \"\"\"\n",
    "    for i in np.unique(y):\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], s=s,\n",
    "                    c=CMAP(i % 10), alpha=alpha,\n",
    "                    marker=MARKERS[i % 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Séparation en ensemble d'entrainement et de test\n",
    "\n",
    "Pour les techniques d'apprentissage supervisé, il est necessaire d'utiliser un ensemble pour apprendre le modèle `X_train, y_train` et un ensemble pour l'évaluer `X_test, y_test`. Créer ces 2 ensembles tirant aléatoirement 60% des donnés pour le train et le reste pour le test. On pourra utiliser soit:\n",
    "* Un mask booleen `mask_train` tiré aléatoirement à partir d'une loi uniforme comparée à `0.5`.\n",
    "* La fonction `np.choice` pour créer un ensemble `id_train`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Entrainement de la plus proche moyenne\n",
    "\n",
    "Calculer les moyennes par classes:\n",
    "* On créera d'abord un tableau vide de taille `n_classes` x `n_dim`.\n",
    "* Calculer les moyennes empirique $\\bar \\mu_i = \\sum_{j \\in C_i} X_j$ de `X_train` pour chaque classe $C_i$ .\n",
    "* Afficher les resultats avec la fonction `plot_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.5 - Prédictions\n",
    "\n",
    "Générer les prédicitons à partir de ce modèle pour l'ensemble `X_test`:\n",
    "* Pour chaque classe, calculer la distance de la moyenne empirique $\\bar \\mu_i$ à tous les points et la stockée dans une matrice `C` de taille `n_points` x `n_classes`.\n",
    "* Pour chaque point, prédite $\\bar y = \\arg\\min_i \\| X - \\bar \\mu_i\\|_2^2$\n",
    "* Afficher les resultats avec la fonction `plot_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Évaluation du model\n",
    "\n",
    "Maintenant que l'on a fait des prédictions, on va calculer l'accuracy du modèle. Évaluer les prédictions du modèle entrainé précédement avec la formule: $P(\\bar y) = \\frac{1}{N}\\sum_{i} 1\\{\\bar y_i = y_i\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 - Fonctions\n",
    "\n",
    "On va maintenant regrouper tout ce qu'on a fait auparavent dans des fonctions.\n",
    "\n",
    "1) Faire une fonction `generate_data` qui prend en entrée `n_points` et `mu` et qui retourne un jeu de donnée `X, y` comme généré dans 2.1. On rappelle que:\n",
    "* `mu` est de taille `n_classes` x `n_dim`.\n",
    "* Pour chaque point, `y` est tiré aléatoirement entre `n_classes`.\n",
    "* `X` est généré selon une loi normale de moyenne `mu[y]` et de variance $\\pmb I$.\n",
    "\n",
    "\n",
    "2) Écrire une fonction `split_train_test` qui prenne en entrée `(X, y)` et retourne 2 ensemble de données `(X_train, y_train)` et `(X_test, y_test)`. On pourra ajouter un paramètre `ratio` (float in [0, 1]) qui donne le rapport de proportion train/test.\n",
    "\n",
    "3) Écrire une fonction `fit` qui prenne en entrée `X_train, y_train` et qui renvoie les centroids de chacune des classes `centroids`.\n",
    "\n",
    "4) Écrire une fonction `predict` qui prenne en entrée `X_test` et `centroids` et qui renvoie en sortie la prédiction `y_pred` du modèle.\n",
    "\n",
    "5) Faire une fonction `score` qui prend en entrée un couple `y` et `y_pred` et qui retourne l'accuracy pour ce couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 - Evaluation du model\n",
    "\n",
    "* Faire jouer les paramètres `n_classes` et `sig`. Quel sont les situations les plus complexes pour le modèle?\n",
    "\n",
    "* Ajouter une ligne dans le second plot avec la performance de la chance.\n",
    "\n",
    "* Est ce que le modèle apprends toujours mieux que la chance? Même pour `sig` petit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 5\n",
    "n_dims = 2\n",
    "n_points = 1000\n",
    "sig = 3\n",
    "\n",
    "\n",
    "# Generate class mean\n",
    "mu = sig * np.random.randn(n_classes, n_dims)\n",
    "\n",
    "list_ratio = np.logspace(-1, 1, 11)\n",
    "\n",
    "X_test, y_test = generate_data(n_points, mu)\n",
    "assert len(np.unique(y_test)) == n_classes\n",
    "\n",
    "plot_data(X_test, y_test, alpha=.1)\n",
    "plot_data(mu, range(n_classes), s=256)\n",
    "\n",
    "plt.figure()\n",
    "score_train = []\n",
    "score_test = []\n",
    "for ratio in list_ratio:\n",
    "    n_train = int(n_points * ratio)\n",
    "    X_train, y_train = generate_data(n_train, mu)\n",
    "    centroids = fit(X_train, y_train)\n",
    "    assert centroids.shape == (n_classes, n_dims)\n",
    "    y_hat_train = predict(X_train, centroids)\n",
    "    y_hat_test = predict(X_test, centroids)\n",
    "    score_train.append(score(y_hat_train, y_train))\n",
    "    score_test.append(score(y_hat_test, y_test))\n",
    "    \n",
    "plt.plot(list_ratio, score_train, label='Train')\n",
    "plt.plot(list_ratio, score_test, label='Test')\n",
    "plt.hlines(score(predict(X_test, mu), y_test),\n",
    "           0, 10, color='k', linestyle='--')\n",
    "plt.xlabel(\"n_train\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Bonus\n",
    "\n",
    "* Rajouter un paramètre `var` à la fonction `generate_data` qui encode la covariance des données. On poura utiliser une matrice aléatoire $A$ et prendre $var = A^top A$ pour avoir une matrice psd. Visualiser les lignes de niveaux des fonctions de repartitions de des différentes classes.\n",
    "\n",
    "* Implémenter l'aglorithme du plus proche centroid: au lieu de prendre la moyenne sur chaque classe, prendre le point dans l'ensemble de train tel que: $\\bar X = \\min_{x \\in X_train} \\sum_i \\|x_i - x\\|_2^2$.\n",
    "\n",
    "* Implémenter l'algorithme des [k-plus proches voisins](https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins). Dans ce cas, l'entrainement `fit` n'existe pas. Pour prédire le label de `x`, trouver les k points `idx_k` les plus proches de `x` dans `X_train` et retourner `y_hat` la classe majoritaire dans `y[idx_k]`. Comment les prédictions varient avec le paramètre `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
