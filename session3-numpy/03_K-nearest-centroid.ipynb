{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions de bases sur `numpy` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1 :** (*Quelques rappels*)  \n",
    "* Générer un tableau de 100 `0`.\n",
    "* Remplacer la valeur 42 par `1`.\n",
    "* Remplacer les 10 premières valeurs par `27`.\n",
    "* Remplacer les 10 dernières valeurs par `-4`.\n",
    "* Remplacer une valeur sur 2 par le double.\n",
    "* Utilisé la méthode `reshape` pour obtenir un tableau de taille `5x20`.\n",
    "* Multiplier une colonne sur 3 par `-1`.\n",
    "* Retrancher `-3` aux valeurs non-nulles.\n",
    "* Calculer la somme des valeurs dans le tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opérations sur les tableaux\n",
    "\n",
    "#### Opérations terme-à-terme\n",
    "\n",
    "**Exercice 2 :** (*Courbe paramétrique*)  \n",
    "Tracer la courbe $x = cos(t)$ et $y=sin(t)$ pour $t \\in [0, 2\\pi]$.  \n",
    "On pourra utiliser la constante $\\pi=$`np.pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "Que donnerait les deux cas suivant?\n",
    "\n",
    "```\n",
    "Image  (3d array): 256 x 256 x 3\n",
    "Scale  (1d array):             3\n",
    "Result (3d array): \n",
    "\n",
    "A      (4d array):  8 x 1 x 6 x 1\n",
    "B      (3d array):      7 x 1 x 5\n",
    "Result (4d array):  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3 :** (*Opération terme-à-terme*)\n",
    "\n",
    "Sans utiliser de boucles (`for/while`) :\n",
    "\n",
    " * Créer une matrice (5x6) aléatoire\n",
    " * Remplacer une colonne sur deux par sa valeur moins le double de la colonne suivante\n",
    " * Remplacer les valeurs négatives par 0 en utilisant un masque binaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4 :** (*Résolution d'un système linéaire*)  \n",
    "Résoudre le systeme d'équation: $\\begin{cases}3x -2y +z &= 10\\\\x +5y + 10z &= 21\\\\y - z &= -5\\\\\\end{cases}$.  \n",
    "(*Astuce:* utiliser la fonction `np.linalg.inv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5 :** (*Blanchiment de donnée*)  \n",
    "Créer un tableau X de taille `100x5` selon une loi normale ($\\sim$ `np.random.randn`).  \n",
    "Soustrayer à chaque colone sa moyenne et la diviser par son écart type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Plus proche centroid\n",
    "\n",
    "Nous allons maintenant implémenter l'agorithme des [plus proches centroids](https://en.wikipedia.org/wiki/Nearest_centroid_classifier) en pure `numpy`, depuis la génération de donnée jusqu'a la visualisation du résultat.  \n",
    "L'idée de cette algorithme est simple:\n",
    "* A partir de l'ensemble d'entrainement $(X^i, y^i)$, calculer pour chaque classe la moyenne des points de cette classe, *i.e.* $\\bar X_l = \\frac{1}{|C_l|} \\sum_{i\\in C_l} X^i$ où $C_l$ est les indices des points appartenant à la classe $l$.\n",
    "* Pour un point de test $X$, lui assigner la classe $y = \\arg\\min_l \\|X - \\bar X_l\\|_2$.\n",
    "\n",
    "Les étapes à réaliser sont:\n",
    "\n",
    "1) Générer des nuages de points en 2D selon des lois normales de moyennes différentes pour chaque classe.\n",
    "\n",
    "2) Visualiser ces nuages de points\n",
    "\n",
    "3) Diviser les données entre un ensemble d'entrainement et un ensemble de test.\n",
    "\n",
    "4) Entrainer le modèle: calculer les centroids pour chacune des classes\n",
    "\n",
    "5) Utiliser le modèle: prédire les classes des points dans l'ensemble de test.\n",
    "\n",
    "6) Évaluer le modèle: donner la précision et le rappel du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Génération des données\n",
    "\n",
    "On va maintenant utiliser les fonctions que l'on a vu précédement pour générer des données pour notre algorithme de plus proche centroid.  \n",
    "* On va créer des données avec 2 classes : $\\{0, 1\\}$.\n",
    "* Pour la classe $i$, on générera les X associés dans $\\mathbb R^2$ selon la loi normale $\\mathcal N(\\mu_i, \\pmb I_2)$.\n",
    "\n",
    "\n",
    "Procédez selon les étapes suivantes:\n",
    "* Créer 2 vecteurs avec en dimension 2 correspondant aux moyennes de chacune des classes. On commencera avec de moyenne deterministe $\\mu_0 = 0$ et $\\mu_1 = [rho, rho]$ avec $rho=3$.\n",
    "* Concaténer `mu0` et `mu1` en un vecteur `mu` de taille `n_classes` x `n_dims` (Indice: utiliser `np.concatenate`).\n",
    "* Générer un vecteur de classes `y` de taille `n_points=1000` avec des valeurs uniforme dans $\\{0, 1\\}$.\n",
    "* Générer un jeu de donné $X$ avec `n_points=1000` points en dimension `n_dim=2` selon la loi $\\mathcal N(0, \\pmb I_2)$. (*Rappel:* tiré selon une loi normal de covariance $\\pmb I_2$ revient à tirer chaque coordonnée selon une loi normale indépendante.)\n",
    "* Ajouter à chaque point la moyenne correspondant à sa classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définion des constantes\n",
    "rho = 3\n",
    "n_dim = 2\n",
    "n_points = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vous de jouer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Visualisation des données\n",
    "\n",
    "Utiliser la fonction `plot_data` pour visualiser les données.  \n",
    "* Afficher les donnés $(X, y)$, avec l'option `alpha=.1`.\n",
    "* Afficher les moyennes avec les mêmes couleurs et l'option `s=256, alpha=1`.\n",
    "Que fait la fonction scatter?\n",
    "\n",
    "Bonus:\n",
    "\n",
    "* Visualiser la fonction de répartition des données à l'aide de `plt.contourf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS = ['^', 's', 'o', 'h', '>', 'v', '*', '+', 'x', '<']\n",
    "CMAP = plt.get_cmap('tab10', 10)\n",
    "\n",
    "def plot_data(X, y, alpha=1, s=36):\n",
    "    \"\"\"Plot the data with color depending on the classes.\n",
    "    \n",
    "    Parameter\n",
    "    =========\n",
    "    X: ndarray, shape (n_points, n_dim)\n",
    "        Point to display\n",
    "    y : ndarray, shape (n_points,)\n",
    "        Classes for each point\n",
    "    alpha: float\n",
    "        Opacity of the points\n",
    "    s: int\n",
    "        Size of the points in pt^2.\n",
    "    \"\"\"\n",
    "    for i in np.unique(y):\n",
    "        plt.scatter(X[y == i, 0], X[y == i, 1], s=s,\n",
    "                    c=CMAP(i % 10), alpha=alpha,\n",
    "                    marker=MARKERS[i % 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Séparation en ensemble d'entrainement et de test\n",
    "\n",
    "Pour les techniques d'apprentissage supervisé, il est necessaire d'utiliser un ensemble pour apprendre le modèle `X_train, y_train` et un ensemble pour l'évaluer `X_test, y_test`. Créer ces 2 ensembles tirant aléatoirement 60% des donnés pour le train et le reste pour le test. On pourra utiliser soit:\n",
    "* Un mask booleen `mask_train` tiré aléatoirement à partir d'une loi uniforme comparée à `0.5`.\n",
    "* La fonction `np.choice` pour créer un ensemble `id_train`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Entrainement de la plus proche moyenne\n",
    "\n",
    "Calculer les moyennes par classes:\n",
    "* On créera d'abord un tableau vide de taille `n_classes` x `n_dim`.\n",
    "* Calculer les moyennes empirique $\\bar \\mu_i = \\sum_{j \\in C_i} X_j$ de `X_train` pour chaque classe $C_i$ .\n",
    "* Afficher les resultats avec la fonction `plot_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.5 - Prédictions\n",
    "\n",
    "Générer les prédicitons à partir de ce modèle pour l'ensemble `X_test`:\n",
    "* Pour chaque classe, calculer la distance de la moyenne empirique $\\bar \\mu_i$ à tous les points et la stockée dans une matrice `C` de taille `n_points` x `n_classes`.\n",
    "* Pour chaque point, prédite $\\bar y = \\arg\\min_i \\| X - \\bar \\mu_i\\|_2^2$\n",
    "* Afficher les resultats avec la fonction `plot_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Évaluation du model\n",
    "\n",
    "Maintenant que l'on a fait des prédictions, on va calculer l'accuracy du modèle. Évaluer les prédictions du modèle entrainé précédement avec la formule: $P(\\bar y) = \\frac{1}{N}\\sum_{i} 1\\{\\bar y_i = y_i\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 - Fonctions\n",
    "\n",
    "On va maintenant regrouper tout ce qu'on a fait auparavent dans des fonctions.\n",
    "\n",
    "1) Faire une fonction `generate_data` qui prend en entrée `n_points` et `mu` et qui retourne un jeu de donnée `X, y` comme généré dans 2.1. On rappelle que:\n",
    "* `mu` est de taille `n_classes` x `n_dim`.\n",
    "* Pour chaque point, `y` est tiré aléatoirement entre `n_classes`.\n",
    "* `X` est généré selon une loi normale de moyenne `mu[y]` et de variance $\\pmb I$.\n",
    "\n",
    "\n",
    "2) Écrire une fonction `split_train_test` qui prenne en entrée `(X, y)` et retourne 2 ensemble de données `(X_train, y_train)` et `(X_test, y_test)`. On pourra ajouter un paramètre `ratio` (float in [0, 1]) qui donne le rapport de proportion train/test.\n",
    "\n",
    "3) Écrire une fonction `fit` qui prenne en entrée `X_train, y_train` et qui renvoie les centroids de chacune des classes `centroids`.\n",
    "\n",
    "4) Écrire une fonction `predict` qui prenne en entrée `X_test` et `centroids` et qui renvoie en sortie la prédiction `y_pred` du modèle.\n",
    "\n",
    "5) Faire une fonction `score` qui prend en entrée un couple `y` et `y_pred` et qui retourne l'accuracy pour ce couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 - Evaluation du model\n",
    "\n",
    "* Faire jouer les paramètres `n_classes` et `sig`. Quel sont les situations les plus complexes pour le modèle?\n",
    "\n",
    "* Ajouter une ligne dans le second plot avec la performance de la chance.\n",
    "\n",
    "* Est ce que le modèle apprends toujours mieux que la chance? Même pour `sig` petit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 5\n",
    "n_dims = 2\n",
    "n_points = 1000\n",
    "sig = 3\n",
    "\n",
    "\n",
    "# Generate class mean\n",
    "mu = sig * np.random.randn(n_classes, n_dims)\n",
    "\n",
    "list_ratio = np.logspace(-1, 1, 11)\n",
    "\n",
    "X_test, y_test = generate_data(n_points, mu)\n",
    "assert len(np.unique(y_test)) == n_classes\n",
    "\n",
    "plot_data(X_test, y_test, alpha=.1)\n",
    "plot_data(mu, range(n_classes), s=256)\n",
    "\n",
    "plt.figure()\n",
    "score_train = []\n",
    "score_test = []\n",
    "for ratio in list_ratio:\n",
    "    n_train = int(n_points * ratio)\n",
    "    X_train, y_train = generate_data(n_train, mu)\n",
    "    centroids = fit(X_train, y_train)\n",
    "    assert centroids.shape == (n_classes, n_dims)\n",
    "    y_hat_train = predict(X_train, centroids)\n",
    "    y_hat_test = predict(X_test, centroids)\n",
    "    score_train.append(score(y_hat_train, y_train))\n",
    "    score_test.append(score(y_hat_test, y_test))\n",
    "    \n",
    "plt.plot(list_ratio, score_train, label='Train')\n",
    "plt.plot(list_ratio, score_test, label='Test')\n",
    "plt.hlines(score(predict(X_test, mu), y_test),\n",
    "           0, 10, color='k', linestyle='--')\n",
    "plt.xlabel(\"n_train\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Bonus\n",
    "\n",
    "* Rajouter un paramètre `var` à la fonction `generate_data` qui encode la covariance des données. On poura utiliser une matrice aléatoire $A$ et prendre $var = A^top A$ pour avoir une matrice psd. Visualiser les lignes de niveaux des fonctions de repartitions de des différentes classes.\n",
    "\n",
    "* Implémenter l'aglorithme du plus proche centroid: au lieu de prendre la moyenne sur chaque classe, prendre le point dans l'ensemble de train tel que: $\\bar X = \\min_{x \\in X_train} \\sum_i \\|x_i - x\\|_2^2$.\n",
    "\n",
    "* Implémenter l'algorithme des [k-plus proches voisins](https://fr.wikipedia.org/wiki/M%C3%A9thode_des_k_plus_proches_voisins). Dans ce cas, l'entrainement `fit` n'existe pas. Pour prédire le label de `x`, trouver les k points `idx_k` les plus proches de `x` dans `X_train` et retourner `y_hat` la classe majoritaire dans `y[idx_k]`. Comment les prédictions varient avec le paramètre `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
